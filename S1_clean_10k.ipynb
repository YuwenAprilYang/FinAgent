{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f59aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException, ConnectionError, HTTPError\n",
    "\n",
    "# SEC requires a valid User-Agent header\n",
    "HEADERS = {\n",
    "    'User-Agent': 'UC Davis Analytics/1.0 (wenjunsong2002@outlook.com)',\n",
    "    'Accept': 'application/json, text/html'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "900b0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, dest_path, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a local path with retries on failure.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            resp.raise_for_status()\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            print(f\"Downloaded: {url} -> {dest_path}\")\n",
    "            return\n",
    "        except (ConnectionError, HTTPError, RequestException) as e:\n",
    "            print(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "            if attempt < retries:\n",
    "                time.sleep(delay)\n",
    "    raise RuntimeError(f\"Failed to download file after {retries} attempts: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af12fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_for_xml(index_url):\n",
    "    \"\"\"\n",
    "    Fetch the EDGAR index page and parse out the XML filename ending with _htm.xml.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(index_url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "    except (ConnectionError, HTTPError, RequestException) as e:\n",
    "        raise RuntimeError(f\"Failed to load index page: {e}\")\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    link = soup.find('a', href=re.compile(r'.+_htm\\.xml$'))\n",
    "    if not link:\n",
    "        raise ValueError('Could not find XML file link on index page')\n",
    "    xml_filename = os.path.basename(link['href'])\n",
    "    base = index_url.rsplit('/', 1)[0]\n",
    "    xml_url = f\"{base}/{xml_filename}\"\n",
    "    return xml_url, xml_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0e5d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content):\n",
    "    \"\"\"\n",
    "    Normalize whitespace in text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", content).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "666cc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_item_robust(text, start_label, end_label):\n",
    "    \"\"\"\n",
    "    Extract text between two markers, skipping the first occurrence if it's in the TOC.\n",
    "    \"\"\"\n",
    "    pattern_start = re.compile(start_label, re.IGNORECASE)\n",
    "    pattern_end = re.compile(end_label, re.IGNORECASE)\n",
    "    starts = list(pattern_start.finditer(text))\n",
    "    if not starts:\n",
    "        return ''\n",
    "    start_idx = starts[1].start() if len(starts) > 1 else starts[0].start()\n",
    "    end_match = pattern_end.search(text[start_idx:])\n",
    "    if not end_match:\n",
    "        return text[start_idx:].strip()\n",
    "    end_idx = start_idx + end_match.start()\n",
    "    return text[start_idx:end_idx].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68e9e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    return {\n",
    "        'item1': extract_item_robust(text, r'Item 1\\.?\\s+Business', r'Item 1A\\.?\\s+Risk Factors'),\n",
    "        'item1a': extract_item_robust(text, r'Item 1A\\.?\\s+Risk Factors', r'Item 1B\\.?\\s+Unresolved Staff Comments'),\n",
    "        'item7': extract_item_robust(text, r'Item 7\\.?\\s+Management.*?Discussion.*?Financial.*?Condition', r'Item 7A\\.?\\s+Quantitative and Qualitative'),\n",
    "        'item7a': extract_item_robust(text, r'Item 7A\\.?\\s+Quantitative and Qualitative', r'Item 8\\.?\\s+Financial Statements')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7406be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cusip_from_submissions(cik):\n",
    "    cik_padded = f\"{int(cik):010d}\"\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        cusips = data.get('filings', {}).get('recent', {}).get('cusip', [])\n",
    "        if cusips:\n",
    "            return cusips[0][:6]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch CUSIP from submissions JSON: {e}\")\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19904d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old\n",
    "def fetch_cusip_from_sc13ga(cik):\n",
    "    \"\"\"\n",
    "    Find CUSIP6 by parsing the latest SC 13G/A beneficial ownership report.\n",
    "    The CUSIP number often appears before the phrase '(CUSIP Number)'.\n",
    "    \"\"\"\n",
    "    cik_padded = f\"{int(cik):010d}\"\n",
    "    subs_url = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n",
    "    try:\n",
    "        resp = requests.get(subs_url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        forms = data['filings']['recent']['form']\n",
    "        accs = data['filings']['recent']['accessionNumber']\n",
    "        docs = data['filings']['recent']['primaryDocument']\n",
    "        cik_no_zero = cik.lstrip('0')\n",
    "        for form, acc, doc in zip(forms, accs, docs):\n",
    "            if form.startswith('SC 13G'):\n",
    "                acc_no_dash = acc.replace('-', '')\n",
    "                url = f\"https://www.sec.gov/Archives/edgar/data/{cik_no_zero}/{acc_no_dash}/{doc}\"\n",
    "                r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "                r.raise_for_status()\n",
    "                text = r.text\n",
    "                # first attempt: number before '(CUSIP Number)'\n",
    "                m = re.search(r'([0-9A-Za-z-]{6,11})\\s*\\(\\s*CUSIP\\s*Number', text, re.IGNORECASE)\n",
    "                if m:\n",
    "                    return m.group(1).replace('-', '')[:6]\n",
    "                # fallback: CUSIP NO. after label\n",
    "                m2 = re.search(r'CUSIP\\s*NO\\.?\\s*([0-9A-Za-z-]{6,11})', text, re.IGNORECASE)\n",
    "                if m2:\n",
    "                    return m2.group(1).replace('-', '')[:6]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch CUSIP from SC 13G/A: {e}\")\n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bab6deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_xml(xml_path):\n",
    "    \"\"\"\n",
    "    Parse the XBRL XML file for metadata: company name, CIK, CUSIP6.\n",
    "    Falls back to submissions JSON or SC 13G/A if CUSIP absent.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(xml_path, 'r', encoding='utf-8') as f:\n",
    "            xml_soup = BeautifulSoup(f, 'lxml-xml')\n",
    "        name_tag = xml_soup.find('dei:EntityRegistrantName') or xml_soup.find('EntityRegistrantName')\n",
    "        cik_tag = xml_soup.find('dei:EntityCentralIndexKey') or xml_soup.find('EntityCentralIndexKey')\n",
    "        cusip_tag = xml_soup.find('dei:CusipNumber') or xml_soup.find('CusipNumber')\n",
    "        names = name_tag.text.strip() if name_tag else ''\n",
    "        cik = cik_tag.text.strip() if cik_tag else ''\n",
    "        cusip6 = cusip_tag.text.strip()[:6] if cusip_tag else ''\n",
    "        if not cusip6 and cik:\n",
    "            cusip6 = fetch_cusip_from_submissions(cik)\n",
    "        if not cusip6 and cik:\n",
    "            cusip6 = fetch_cusip_from_sc13ga(cik)\n",
    "        return { 'names': names, 'cik': cik, 'cusip6': cusip6 }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse XML metadata: {e}\")\n",
    "        return { 'names': '', 'cik': '', 'cusip6': '' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "982655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean(index_url, output_json='ntap_10k_cleaned.json'):\n",
    "    try:\n",
    "        xml_url, xml_filename = parse_index_for_xml(index_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing index page: {e}\")\n",
    "        return\n",
    "    os.makedirs('download', exist_ok=True)\n",
    "    xml_path = os.path.join('download', xml_filename)\n",
    "    try:\n",
    "        download_file(xml_url, xml_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading XML file: {e}\")\n",
    "        return\n",
    "    htm_filename = xml_filename.replace('_htm.xml', '.htm')\n",
    "    htm_url = xml_url.replace(xml_filename, htm_filename)\n",
    "    htm_path = os.path.join('download', htm_filename)\n",
    "    try:\n",
    "        download_file(htm_url, htm_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading HTML file: {e}\")\n",
    "        return\n",
    "    try:\n",
    "        with open(htm_path, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        full_text = soup.get_text(separator=' ')\n",
    "        cleaned = clean_text(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML text: {e}\")\n",
    "        return\n",
    "    sections = extract_sections(cleaned)\n",
    "    meta = extract_metadata_from_xml(xml_path)\n",
    "    # Construct official iXBRL URL as source\n",
    "    # xml_url like https://www.sec.gov/Archives/.../indexdir/filename_htm.xml\n",
    "    # derive base archive path\n",
    "    archive_prefix = xml_url.split('https://www.sec.gov')[-1].rsplit('/', 1)[0]\n",
    "    ix_htm_url = f\"https://www.sec.gov/ix?doc={archive_prefix}/{htm_filename}\"\n",
    "    meta['source'] = ix_htm_url\n",
    "    # Build official iXBRL source URL for clickable link\n",
    "    archive_path = xml_url.split('https://www.sec.gov')[-1].rsplit('/', 1)[0]\n",
    "    ix_htm_url = f\"https://www.sec.gov/ix?doc={archive_path}/{htm_filename}\"\n",
    "    meta['source'] = ix_htm_url\n",
    "    result = { **meta, **sections }\n",
    "    try:\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved cleaned data to {output_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0eb67fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL: https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/0000320193-24-000123-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928_htm.xml -> download/aapl-20240928_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm -> download/aapl-20240928.htm\n",
      "Saved cleaned data to aapl_10k_cleaned.json\n",
      "Processing JPM: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/0000019617-25-000270-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/jpm-20241231_htm.xml -> download/jpm-20241231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/jpm-20241231.htm -> download/jpm-20241231.htm\n",
      "Saved cleaned data to jpm_10k_cleaned.json\n",
      "Processing JNJ: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/0000200406-25-000038-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/jnj-20241229_htm.xml -> download/jnj-20241229_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/jnj-20241229.htm -> download/jnj-20241229.htm\n",
      "Saved cleaned data to jnj_10k_cleaned.json\n",
      "Processing XOM: https://www.sec.gov/Archives/edgar/data/34088/000003408825000010/0000034088-25-000010-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408825000010/xom-20241231_htm.xml -> download/xom-20241231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408825000010/xom-20241231.htm -> download/xom-20241231.htm\n",
      "Saved cleaned data to xom_10k_cleaned.json\n",
      "Processing WMT: https://www.sec.gov/Archives/edgar/data/104169/000010416925000021/0000104169-25-000021-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/104169/000010416925000021/wmt-20250131_htm.xml -> download/wmt-20250131_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/104169/000010416925000021/wmt-20250131.htm -> download/wmt-20250131.htm\n",
      "Saved cleaned data to wmt_10k_cleaned.json\n",
      "Processing TSLA: https://www.sec.gov/Archives/edgar/data/1318605/000162828025003063/0001628280-25-003063-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1318605/000162828025003063/tsla-20241231_htm.xml -> download/tsla-20241231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1318605/000162828025003063/tsla-20241231.htm -> download/tsla-20241231.htm\n",
      "Saved cleaned data to tsla_10k_cleaned.json\n",
      "Processing PLD: https://www.sec.gov/Archives/edgar/data/1045609/000095017025021272/0000950170-25-021272-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1045609/000095017025021272/pld-20241231_htm.xml -> download/pld-20241231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1045609/000095017025021272/pld-20241231.htm -> download/pld-20241231.htm\n",
      "Saved cleaned data to pld_10k_cleaned.json\n",
      "Processing BA: https://www.sec.gov/Archives/edgar/data/12927/000001292725000015/0000012927-25-000015-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/12927/000001292725000015/ba-20241231_htm.xml -> download/ba-20241231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/12927/000001292725000015/ba-20241231.htm -> download/ba-20241231.htm\n",
      "Saved cleaned data to ba_10k_cleaned.json\n",
      "Processing NFLX: https://www.sec.gov/Archives/edgar/data/1065280/000106528025000044/0001065280-25-000044-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1065280/000106528025000044/nflx-20241231_htm.xml -> download/nflx-20241231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1065280/000106528025000044/nflx-20241231.htm -> download/nflx-20241231.htm\n",
      "Saved cleaned data to nflx_10k_cleaned.json\n",
      "Processing NVDA: https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/0001045810-25-000023-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126_htm.xml -> download/nvda-20250126_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm -> download/nvda-20250126.htm\n",
      "Saved cleaned data to nvda_10k_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # List of tickers to process\n",
    "    tickers = [\n",
    "        'AAPL', 'JPM', 'JNJ', 'XOM', 'WMT',\n",
    "        'TSLA', 'PLD', 'BA', 'NFLX', 'NVDA'\n",
    "    ]\n",
    "    # Fetch mapping of ticker to CIK\n",
    "    def get_cik_mapping():\n",
    "        mapping_url = 'https://www.sec.gov/files/company_tickers.json'\n",
    "        resp = requests.get(mapping_url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return {v['ticker']: v['cik_str'] for v in data.values()}\n",
    "\n",
    "    # Find latest 10-K accession number from submissions JSON\n",
    "    def find_latest_10k_accession(cik):\n",
    "        padded = f\"{int(cik):010d}\"\n",
    "        subs_url = f\"https://data.sec.gov/submissions/CIK{padded}.json\"\n",
    "        resp = requests.get(subs_url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        forms = data['filings']['recent']['form']\n",
    "        accs = data['filings']['recent']['accessionNumber']\n",
    "        for form, acc in zip(forms, accs):\n",
    "            if form == '10-K':\n",
    "                return acc\n",
    "        raise ValueError(f\"No 10-K found for CIK {cik}\")\n",
    "\n",
    "    cik_map = get_cik_mapping()\n",
    "    for ticker in tickers:\n",
    "        cik = cik_map.get(ticker)\n",
    "        if not cik:\n",
    "            print(f\"CIK not found for ticker {ticker}, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            acc = find_latest_10k_accession(cik)\n",
    "            acc_nodash = acc.replace('-', '')\n",
    "            index_filename = f\"{acc}-index.html\"\n",
    "            index_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc_nodash}/{index_filename}\"\n",
    "            out_json = f\"{ticker.lower()}_10k_cleaned.json\"\n",
    "            print(f\"Processing {ticker}: {index_url}\")\n",
    "            extract_and_clean(index_url, output_json=out_json)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed processing {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa59812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

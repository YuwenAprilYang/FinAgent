{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f59aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException, ConnectionError, HTTPError\n",
    "\n",
    "# SEC requires a valid User-Agent header\n",
    "HEADERS = {\n",
    "    'User-Agent': 'UC Davis Analytics/1.0 (wenjunsong2002@outlook.com)',\n",
    "    'Accept': 'application/json, text/html'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "900b0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, dest_path, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a local path with retries on failure.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            resp.raise_for_status()\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            print(f\"Downloaded: {url} -> {dest_path}\")\n",
    "            return\n",
    "        except (ConnectionError, HTTPError, RequestException) as e:\n",
    "            print(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "            if attempt < retries:\n",
    "                time.sleep(delay)\n",
    "    raise RuntimeError(f\"Failed to download file after {retries} attempts: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af12fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_for_xml(index_url):\n",
    "    \"\"\"\n",
    "    Fetch the EDGAR index page and parse out the XML filename ending with _htm.xml.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(index_url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "    except (ConnectionError, HTTPError, RequestException) as e:\n",
    "        raise RuntimeError(f\"Failed to load index page: {e}\")\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    link = soup.find('a', href=re.compile(r'.+_htm\\.xml$'))\n",
    "    if not link:\n",
    "        raise ValueError('Could not find XML file link on index page')\n",
    "    xml_filename = os.path.basename(link['href'])\n",
    "    base = index_url.rsplit('/', 1)[0]\n",
    "    xml_url = f\"{base}/{xml_filename}\"\n",
    "    return xml_url, xml_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0e5d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content):\n",
    "    \"\"\"\n",
    "    Normalize whitespace in text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", content).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "666cc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_item_robust(text, start_label, end_label):\n",
    "    \"\"\"\n",
    "    Extract text between two markers, skipping the first occurrence if it's in the TOC.\n",
    "    \"\"\"\n",
    "    pattern_start = re.compile(start_label, re.IGNORECASE)\n",
    "    pattern_end = re.compile(end_label, re.IGNORECASE)\n",
    "    starts = list(pattern_start.finditer(text))\n",
    "    if not starts:\n",
    "        return ''\n",
    "    start_idx = starts[1].start() if len(starts) > 1 else starts[0].start()\n",
    "    end_match = pattern_end.search(text[start_idx:])\n",
    "    if not end_match:\n",
    "        return text[start_idx:].strip()\n",
    "    end_idx = start_idx + end_match.start()\n",
    "    return text[start_idx:end_idx].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68e9e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    return {\n",
    "        'item1': extract_item_robust(text, r'Item 1\\.?\\s+Business', r'Item 1A\\.?\\s+Risk Factors'),\n",
    "        'item1a': extract_item_robust(text, r'Item 1A\\.?\\s+Risk Factors', r'Item 1B\\.?\\s+Unresolved Staff Comments'),\n",
    "        'item7': extract_item_robust(text, r'Item 7\\.?\\s+Management.*?Discussion.*?Financial.*?Condition', r'Item 7A\\.?\\s+Quantitative and Qualitative'),\n",
    "        'item7a': extract_item_robust(text, r'Item 7A\\.?\\s+Quantitative and Qualitative', r'Item 8\\.?\\s+Financial Statements')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7406be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cusip_from_submissions(cik):\n",
    "    cik_padded = f\"{int(cik):010d}\"\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        cusips = data.get('filings', {}).get('recent', {}).get('cusip', [])\n",
    "        if cusips:\n",
    "            return cusips[0][:6]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch CUSIP from submissions JSON: {e}\")\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19904d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old\n",
    "def fetch_cusip_from_sc13ga(cik):\n",
    "    \"\"\"\n",
    "    Find CUSIP6 by parsing the latest SC 13G/A beneficial ownership report.\n",
    "    The CUSIP number often appears before the phrase '(CUSIP Number)'.\n",
    "    \"\"\"\n",
    "    cik_padded = f\"{int(cik):010d}\"\n",
    "    subs_url = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n",
    "    try:\n",
    "        resp = requests.get(subs_url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        forms = data['filings']['recent']['form']\n",
    "        accs = data['filings']['recent']['accessionNumber']\n",
    "        docs = data['filings']['recent']['primaryDocument']\n",
    "        cik_no_zero = cik.lstrip('0')\n",
    "        for form, acc, doc in zip(forms, accs, docs):\n",
    "            if form.startswith('SC 13G'):\n",
    "                acc_no_dash = acc.replace('-', '')\n",
    "                url = f\"https://www.sec.gov/Archives/edgar/data/{cik_no_zero}/{acc_no_dash}/{doc}\"\n",
    "                r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "                r.raise_for_status()\n",
    "                text = r.text\n",
    "                # first attempt: number before '(CUSIP Number)'\n",
    "                m = re.search(r'([0-9A-Za-z-]{6,11})\\s*\\(\\s*CUSIP\\s*Number', text, re.IGNORECASE)\n",
    "                if m:\n",
    "                    return m.group(1).replace('-', '')[:6]\n",
    "                # fallback: CUSIP NO. after label\n",
    "                m2 = re.search(r'CUSIP\\s*NO\\.?\\s*([0-9A-Za-z-]{6,11})', text, re.IGNORECASE)\n",
    "                if m2:\n",
    "                    return m2.group(1).replace('-', '')[:6]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch CUSIP from SC 13G/A: {e}\")\n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_xml(xml_path):\n",
    "    \"\"\"\n",
    "    Parse the XBRL XML file for metadata: company name, CIK, CUSIP6.\n",
    "    Falls back to submissions JSON or SC 13G/A if CUSIP absent.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(xml_path, 'r', encoding='utf-8') as f:\n",
    "            xml_soup = BeautifulSoup(f, 'lxml-xml')\n",
    "        name_tag = xml_soup.find('dei:EntityRegistrantName') or xml_soup.find('EntityRegistrantName')\n",
    "        cik_tag = xml_soup.find('dei:EntityCentralIndexKey') or xml_soup.find('EntityCentralIndexKey')\n",
    "        cusip_tag = xml_soup.find('dei:CusipNumber') or xml_soup.find('CusipNumber')\n",
    "        names = name_tag.text.strip() if name_tag else ''\n",
    "        cik = cik_tag.text.strip() if cik_tag else ''\n",
    "        cusip6 = cusip_tag.text.strip()[:6] if cusip_tag else ''\n",
    "        if not cusip6 and cik:\n",
    "            cusip6 = fetch_cusip_from_submissions(cik)\n",
    "        if not cusip6 and cik:\n",
    "            cusip6 = fetch_cusip_from_sc13ga(cik)\n",
    "\n",
    "        \n",
    "        return { 'names': names, 'cik': cik, 'cusip6': cusip6 }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse XML metadata: {e}\")\n",
    "        return { 'names': '', 'cik': '', 'cusip6': '' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "982655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean(index_url, output_json='ntap_10k_cleaned.json'):\n",
    "    try:\n",
    "        xml_url, xml_filename = parse_index_for_xml(index_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing index page: {e}\")\n",
    "        return\n",
    "    os.makedirs('download', exist_ok=True)\n",
    "    xml_path = os.path.join('download', xml_filename)\n",
    "    try:\n",
    "        download_file(xml_url, xml_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading XML file: {e}\")\n",
    "        return\n",
    "    htm_filename = xml_filename.replace('_htm.xml', '.htm')\n",
    "    htm_url = xml_url.replace(xml_filename, htm_filename)\n",
    "    htm_path = os.path.join('download', htm_filename)\n",
    "    try:\n",
    "        download_file(htm_url, htm_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading HTML file: {e}\")\n",
    "        return\n",
    "    try:\n",
    "        with open(htm_path, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        full_text = soup.get_text(separator=' ')\n",
    "        cleaned = clean_text(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML text: {e}\")\n",
    "        return\n",
    "    sections = extract_sections(cleaned)\n",
    "    meta = extract_metadata_from_xml(xml_path)\n",
    "    # Construct official iXBRL URL as source\n",
    "    # xml_url like https://www.sec.gov/Archives/.../indexdir/filename_htm.xml\n",
    "    # derive base archive path\n",
    "    archive_prefix = xml_url.split('https://www.sec.gov')[-1].rsplit('/', 1)[0]\n",
    "    ix_htm_url = f\"https://www.sec.gov/ix?doc={archive_prefix}/{htm_filename}\"\n",
    "    meta['source'] = ix_htm_url\n",
    "    # Build official iXBRL source URL for clickable link\n",
    "    archive_path = xml_url.split('https://www.sec.gov')[-1].rsplit('/', 1)[0]\n",
    "    ix_htm_url = f\"https://www.sec.gov/ix?doc={archive_path}/{htm_filename}\"\n",
    "    meta['source'] = ix_htm_url\n",
    "    result = { **meta, **sections }\n",
    "    try:\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved cleaned data to {output_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0545ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik_mapping():\n",
    "    mapping_url = 'https://www.sec.gov/files/company_tickers.json'\n",
    "    resp = requests.get(mapping_url, headers=HEADERS, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return {v['ticker']: v['cik_str'] for v in data.values()}\n",
    "\n",
    "def find_10k_accessions_by_year(cik):\n",
    "    padded = f\"{int(cik):010d}\"\n",
    "    subs_url = f\"https://data.sec.gov/submissions/CIK{padded}.json\"\n",
    "    resp = requests.get(subs_url, headers=HEADERS, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    forms = data['filings']['recent']['form']\n",
    "    accs = data['filings']['recent']['accessionNumber']\n",
    "    dates = data['filings']['recent']['filingDate']\n",
    "    \n",
    "    year_map = {}\n",
    "    for form, acc, date in zip(forms, accs, dates):\n",
    "        if form == '10-K':\n",
    "            year = date[:4]\n",
    "            year_map.setdefault(year, []).append(acc)\n",
    "    return year_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0eb67fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing AAPL 2014: https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing AAPL 2015: https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/0001193125-15-356351-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing AAPL 2016: https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/0001628280-16-020309-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing AAPL 2017: https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/0000320193-17-000070-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing AAPL 2018: https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/0000320193-18-000145-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing AAPL 2019: https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/0000320193-19-000119-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/a10-k20199282019_htm.xml -> download/a10-k20199282019_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/a10-k20199282019.htm -> download/a10-k20199282019.htm\n",
      "Saved cleaned data to output/AAPL/AAPL_10k_2019.json\n",
      "\n",
      "Processing AAPL 2020: https://www.sec.gov/Archives/edgar/data/320193/000032019320000096/0000320193-20-000096-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019320000096/aapl-20200926_htm.xml -> download/aapl-20200926_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019320000096/aapl-20200926.htm -> download/aapl-20200926.htm\n",
      "Saved cleaned data to output/AAPL/AAPL_10k_2020.json\n",
      "\n",
      "Processing AAPL 2021: https://www.sec.gov/Archives/edgar/data/320193/000032019321000105/0000320193-21-000105-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019321000105/aapl-20210925_htm.xml -> download/aapl-20210925_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019321000105/aapl-20210925.htm -> download/aapl-20210925.htm\n",
      "Saved cleaned data to output/AAPL/AAPL_10k_2021.json\n",
      "\n",
      "Processing AAPL 2022: https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/0000320193-22-000108-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/aapl-20220924_htm.xml -> download/aapl-20220924_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm -> download/aapl-20220924.htm\n",
      "Saved cleaned data to output/AAPL/AAPL_10k_2022.json\n",
      "\n",
      "Processing AAPL 2023: https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/0000320193-23-000106-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/aapl-20230930_htm.xml -> download/aapl-20230930_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/aapl-20230930.htm -> download/aapl-20230930.htm\n",
      "Saved cleaned data to output/AAPL/AAPL_10k_2023.json\n",
      "\n",
      "Processing AAPL 2024: https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/0000320193-24-000123-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928_htm.xml -> download/aapl-20240928_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm -> download/aapl-20240928.htm\n",
      "Saved cleaned data to output/AAPL/AAPL_10k_2024.json\n",
      "\n",
      "Processing JPM 2025: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/0000019617-25-000270-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/jpm-20241231_htm.xml -> download/jpm-20241231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/jpm-20241231.htm -> download/jpm-20241231.htm\n",
      "Saved cleaned data to output/JPM/JPM_10k_2025.json\n",
      "\n",
      "Processing JNJ 2017: https://www.sec.gov/Archives/edgar/data/200406/000020040617000006/0000200406-17-000006-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing JNJ 2018: https://www.sec.gov/Archives/edgar/data/200406/000020040618000005/0000200406-18-000005-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing JNJ 2019: https://www.sec.gov/Archives/edgar/data/200406/000020040619000009/0000200406-19-000009-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing JNJ 2020: https://www.sec.gov/Archives/edgar/data/200406/000020040620000010/0000200406-20-000010-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040620000010/form10-k20191229_htm.xml -> download/form10-k20191229_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040620000010/form10-k20191229.htm -> download/form10-k20191229.htm\n",
      "Saved cleaned data to output/JNJ/JNJ_10k_2020.json\n",
      "\n",
      "Processing JNJ 2021: https://www.sec.gov/Archives/edgar/data/200406/000020040621000008/0000200406-21-000008-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040621000008/jnj-20210103_htm.xml -> download/jnj-20210103_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040621000008/jnj-20210103.htm -> download/jnj-20210103.htm\n",
      "Saved cleaned data to output/JNJ/JNJ_10k_2021.json\n",
      "\n",
      "Processing JNJ 2022: https://www.sec.gov/Archives/edgar/data/200406/000020040622000022/0000200406-22-000022-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040622000022/jnj-20220102_htm.xml -> download/jnj-20220102_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040622000022/jnj-20220102.htm -> download/jnj-20220102.htm\n",
      "Saved cleaned data to output/JNJ/JNJ_10k_2022.json\n",
      "\n",
      "Processing JNJ 2023: https://www.sec.gov/Archives/edgar/data/200406/000020040623000016/0000200406-23-000016-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040623000016/jnj-20230101_htm.xml -> download/jnj-20230101_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040623000016/jnj-20230101.htm -> download/jnj-20230101.htm\n",
      "Saved cleaned data to output/JNJ/JNJ_10k_2023.json\n",
      "\n",
      "Processing JNJ 2024: https://www.sec.gov/Archives/edgar/data/200406/000020040624000013/0000200406-24-000013-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040624000013/jnj-20231231_htm.xml -> download/jnj-20231231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040624000013/jnj-20231231.htm -> download/jnj-20231231.htm\n",
      "Saved cleaned data to output/JNJ/JNJ_10k_2024.json\n",
      "\n",
      "Processing JNJ 2025: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/0000200406-25-000038-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/jnj-20241229_htm.xml -> download/jnj-20241229_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/jnj-20241229.htm -> download/jnj-20241229.htm\n",
      "Saved cleaned data to output/JNJ/JNJ_10k_2025.json\n",
      "\n",
      "Processing XOM 2019: https://www.sec.gov/Archives/edgar/data/34088/000003408819000010/0000034088-19-000010-index.html\n",
      "Error parsing index page: Could not find XML file link on index page\n",
      "\n",
      "Processing XOM 2020: https://www.sec.gov/Archives/edgar/data/34088/000003408820000016/0000034088-20-000016-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408820000016/xom10k2019_htm.xml -> download/xom10k2019_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408820000016/xom10k2019.htm -> download/xom10k2019.htm\n",
      "Saved cleaned data to output/XOM/XOM_10k_2020.json\n",
      "\n",
      "Processing XOM 2021: https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/0000034088-21-000012-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231_htm.xml -> download/xom-20201231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm -> download/xom-20201231.htm\n",
      "Saved cleaned data to output/XOM/XOM_10k_2021.json\n",
      "\n",
      "Processing XOM 2022: https://www.sec.gov/Archives/edgar/data/34088/000003408822000011/0000034088-22-000011-index.html\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408822000011/xom-20211231_htm.xml -> download/xom-20211231_htm.xml\n",
      "Downloaded: https://www.sec.gov/Archives/edgar/data/34088/000003408822000011/xom-20211231.htm -> download/xom-20211231.htm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m             out_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_10k_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m             extract_and_clean(index_url, output_json\u001b[38;5;241m=\u001b[39mout_json)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[74], line 32\u001b[0m, in \u001b[0;36mextract_and_clean\u001b[0;34m(index_url, output_json)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     31\u001b[0m sections \u001b[38;5;241m=\u001b[39m extract_sections(cleaned)\n\u001b[0;32m---> 32\u001b[0m meta \u001b[38;5;241m=\u001b[39m extract_metadata_from_xml(xml_path)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Construct official iXBRL URL as source\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# xml_url like https://www.sec.gov/Archives/.../indexdir/filename_htm.xml\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# derive base archive path\u001b[39;00m\n\u001b[1;32m     36\u001b[0m archive_prefix \u001b[38;5;241m=\u001b[39m xml_url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.sec.gov\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[73], line 20\u001b[0m, in \u001b[0;36mextract_metadata_from_xml\u001b[0;34m(xml_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m         cusip6 \u001b[38;5;241m=\u001b[39m fetch_cusip_from_sc13ga(cik)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cusip6 \u001b[38;5;129;01mand\u001b[39;00m cik:\n\u001b[0;32m---> 20\u001b[0m         cusip6 \u001b[38;5;241m=\u001b[39m fetch_cusip_from_8k(cik)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m: names, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcik\u001b[39m\u001b[38;5;124m'\u001b[39m: cik, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcusip6\u001b[39m\u001b[38;5;124m'\u001b[39m: cusip6 }\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[72], line 19\u001b[0m, in \u001b[0;36mfetch_cusip_from_8k\u001b[0;34m(cik)\u001b[0m\n\u001b[1;32m     17\u001b[0m acc_no_dash \u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.sec.gov/Archives/edgar/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcik_no_zero\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_no_dash\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mHEADERS, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m     20\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    717\u001b[0m     conn,\n\u001b[1;32m    718\u001b[0m     method,\n\u001b[1;32m    719\u001b[0m     url,\n\u001b[1;32m    720\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    721\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    722\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    723\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1061\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1061\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1064\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1065\u001b[0m         (\n\u001b[1;32m   1066\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1072\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_default_certs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[38;5;241m=\u001b[39mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    436\u001b[0m     default_ssl_context\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mversion() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    440\u001b[0m ):  \u001b[38;5;66;03m# Defensive:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py:458\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    446\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    455\u001b[0m     )\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 458\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    459\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py:502\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[1;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[1;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[1;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[1;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m    525\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1073\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1075\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tickers = [\n",
    "        'AAPL', 'JPM', 'JNJ', 'XOM', 'WMT',\n",
    "        'TSLA', 'PLD', 'BA', 'NFLX', 'NVDA'\n",
    "    ]\n",
    "    cik_map = get_cik_mapping()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        cik = cik_map.get(ticker)\n",
    "        if not cik:\n",
    "            print(f\"CIK not found for ticker {ticker}, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            year_to_accessions = find_10k_accessions_by_year(cik)\n",
    "            for year, accessions in sorted(year_to_accessions.items()):\n",
    "                for acc in accessions:\n",
    "                    acc_nodash = acc.replace('-', '')\n",
    "                    index_filename = f\"{acc}-index.html\"\n",
    "                    index_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc_nodash}/{index_filename}\"\n",
    "                    out_dir = f\"output/{ticker}\"\n",
    "                    os.makedirs(out_dir, exist_ok=True)\n",
    "                    out_json = f\"{out_dir}/{ticker}_10k_{year}.json\"\n",
    "                    print(f\"\\nProcessing {ticker} {year}: {index_url}\")\n",
    "                    extract_and_clean(index_url, output_json=out_json)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed processing {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78361ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
